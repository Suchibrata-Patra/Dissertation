x = summary(reduced_model_1)
x
y = x$coefficients
y
z = as.data.frame(y[,4])
z = as.data.frame(y[,4]);z
p = z[,];p
q = which.max(p) ; q
# ======================================================== ##
#                    Final Code                            ##
# ======================================================== ##
rm(list=ls())#
#install.packages("lattice")#
# install.packages("caret")#
#install.packages("pROC")#
#install.packages("randomForest")#
#install.packages("olsrr")#
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
attach(data)#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]#
# = = = = = = = = = = = #
# Correlation Heatmap#
# = = = = = = = = = = = #
correlation_matrix = cor(data)#
melted_correlation = melt(correlation_matrix)#
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +#
  geom_tile() + #
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +#
  scale_fill_gradient2(low = "#58390b", high = "#0f423c", midpoint = 0, limit = c(-1,1), name="Correlation", mid = "#FFFFFF") +#
  theme_minimal() +#
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +#
  coord_fixed() +#
  labs(x = "Variables", y = "Variables")#
# = = = = = = = = = = = = =#
# Converting into factors#
# = = = = = = = = = = = = =#
data$male = as.factor(data$male)#
data$education = as.factor(data$education)#
data$currentSmoker = as.factor(data$currentSmoker)#
data$prevalentStroke = as.factor(data$prevalentStroke)#
data$prevalentHyp = as.factor(data$prevalentHyp)#
data$diabetes = as.factor(data$diabetes)#
data$TenYearCHD = as.factor(data$TenYearCHD)#
model = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# = = = = = = = = = = = = = #
# Plottting the VIF Values#
# = = = = = = = = = = = = =#
#
vif_values = vif(model)#
vif_df = data.frame(Variable = names(vif_values), VIF = unname(vif_values))#
#
# Create the bar chart using ggplot#
ggplot(vif_df, aes(x = Variable, y = VIF, fill = VIF)) +#
  geom_bar(stat = "identity", color = "steelblue") +#
  geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +#
  theme_minimal() +#
  labs(title = "Plotting of the VIF Values", y = "VIF Value", x = "") +#
  coord_cartesian(ylim = c(0, max(vif_df$VIF) + 1)) + # Adjust y-axis limits#
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Angle the x-axis text#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#  Code for testing the significance of the predictor variables#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#
x = summary(model)#
y = x$coefficients#
estimates = y[,1][-c(1,18)]#
se = y[,2][-c(1,18)]#
walds_t = estimates/sqrt(se)#
significance = numeric(16)#
for(i in 1:16){#
	if(abs(walds_t[i])>1.96){#
		significance[i]= "Not Significant"#
	}else{#
		significance[i]="Significant"#
	}#
}#
data.frame(names(training_data),estimates,se,walds_t,significance)#
#Findig Out the Odds Ratio#
Odds_Ratio = exp(estimates)#
p_values = 2*(1- qnorm(estimates)) ; p_values#
data.frame(names(training_data),exp(Odds_Ratio),p_values)
detach(data)
detach(data)
detach(data)
detach(data)
detach(data)
detach(data)
detach(data)
detach(data)
detach(data)
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")
attach(data)
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))
x = summary(fullmodel)
y = x$coefficients
z = as.data.frame(y[,4])
p = z[,]
q = which.max(p) ; q
x
q
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_data_1 = training_data[,-q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_model_1
fullmodel
summary(fullmodel)
summary(fullmodel)
x = summary(reduced_model_1)
x
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_1 = training_data[,-q] #
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)
x = summary(reduced_model_1)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_2 = reduced_data_1[,-q]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)
x = summary(fullmodel)
x
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_1 = training_data[,-q] #
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)
reduced_data_1 = training_data[-q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_data_1 = training_data[,-q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
# Assuming 'x' contains the summary of 'fullmodel'#
x = summary(fullmodel)#
y = x$coefficients#
#
# Identify p-values and convert to a data frame#
z = as.data.frame(y[, 4])#
#
# Instead of using the index to remove, identify the variable's name#
# Note: '+1' in 'coef_names[q + 1]' is not needed because we directly use the names from coefficients excluding the Intercept#
coef_names = names(y)[-1]  # Exclude intercept from names#
highest_p_value_name = coef_names[which.max(z[,1])]#
#
# Now, remove the column by name, not by index#
reduced_data_1 = training_data[, !names(training_data) %in% highest_p_value_name] #
#
# Create the reduced model without the variable with the highest p-value#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
#
# Display the summary of the reduced model#
summary(reduced_model_1)
reduced_data_1 = training_data[!q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_data_1 = training_data[q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_data_1 = training_data[q,2]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_data_1 = training_data[1,q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_data_1 = training_data[q,q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_data_1 = training_data[-q,]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_data_1 = training_data[,q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
reduced_data_1 = training_data[-q,]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
reduced_data_1 = training_data[,-q]
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))
summary(reduced_model_1)
# Fit the full model#
fullmodel <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Get the summary of the full model#
x <- summary(fullmodel)#
#
# Extract p-values#
p_values <- x$coefficients[, 4]#
#
# Find the index of the max p-value excluding the intercept#
max_p_value_index <- which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept#
#
# Get the name of the variable to remove#
variable_to_remove <- names(p_values)[max_p_value_index]#
#
# Remove the variable from the dataset#
reduced_data <- training_data[, !names(training_data) %in% variable_to_remove]#
#
# Fit the model again with the reduced dataset#
reduced_model <- glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))#
#
# Display the summary of the reduced model#
summary(reduced_model)
x = summary(fullmodel)
x = summary(fullmodel);x
rm(list=ls())
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]
x = summary(model)
y = x$coefficients
estimates = y[,1][-c(1,18)]
rm(list=ls())
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))
x = summary(fullmodel)
x
y = x$coefficients
p_values = x$coefficients[, 4]
max_p_value_index = which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept
variable_to_remove = names(p_values)[max_p_value_index]
reduced_data = training_data[, !names(training_data) %in% variable_to_remove]
reduced_model = glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))
reduced_model = glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))
summary(reduced_model)
x = summary(fullmodel)
x
x = summary(reduced_model_1)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept#
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_2 = training_data[, !names(reduced_data_1) %in% variable_to_remove]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)
x = summary(fullmodel)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept#
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_1 = training_data[, !names(training_data) %in% variable_to_remove]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))#
summary(reduced_model_1)
x = summary(reduced_model_1)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept#
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_2 = training_data[, !names(reduced_data_1) %in% variable_to_remove]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)
summary(fullmodel)
x = summary(fullmodel)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1 #
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_1 = training_data[, !names(training_data) %in% variable_to_remove]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)
x = summary(reduced_model_1)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1#
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_2 = training_data[, !names(reduced_data_1) %in% variable_to_remove]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_Reduced_model_2 = 2138.2
x = summary(fullmodel)#
y = x$coefficients#
p_values = x$coefficients[, 4]#
max_p_value_index = which.max(p_values[-1]) + 1 #
variable_to_remove = names(p_values)[max_p_value_index]#
#
reduced_data_1 = training_data[, !names(training_data) %in% variable_to_remove]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)
# Fit the full model#
fullmodel <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Get the summary of the full model#
x <- summary(fullmodel)#
#
# Extract p-values#
p_values <- x$coefficients[, 4]#
#
# Find the index of the max p-value excluding the intercept#
max_p_value_index <- which.max(p_values[-1]) + 1  # +1 to adjust for excluding the intercept#
#
# Get the name of the variable to remove#
variable_to_remove <- names(p_values)[max_p_value_index]#
#
# Remove the variable from the dataset#
reduced_data <- training_data[, !names(training_data) %in% variable_to_remove]#
#
# Fit the model again with the reduced dataset#
reduced_model <- glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))#
#
# Display the summary of the reduced model#
summary(reduced_model)
summary(fullmodel)
summary(reduced_model_2)
summary(reduced_model_1)
# Fit the full model#
fullmodel <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Get the summary of the full model#
x <- summary(fullmodel)#
#
# Initialize the variable for the best AIC#
best_AIC <- AIC(fullmodel)#
#
# Perform backward elimination#
for (i in 1:length(x$coefficients)) {#
  # Get the summary of the current model#
  x <- summary(fullmodel)#
  # Extract p-values#
  p_values <- x$coefficients[, 4]#
  # Find the index of the max p-value excluding the intercept#
  max_p_value_index <- which.max(p_values[-1]) + 1#
  # Get the name of the variable to remove#
  variable_to_remove <- names(p_values)[max_p_value_index]#
  # Remove the variable from the dataset#
  reduced_data <- training_data[, !names(training_data) %in% variable_to_remove]#
  # Fit the model again with the reduced dataset#
  reduced_model <- glm(TenYearCHD ~ ., data = reduced_data, family = binomial(link = "logit"))#
  # Calculate AIC for the reduced model#
  AIC_reduced <- AIC(reduced_model)#
  # If AIC for the reduced model is better, update the best AIC and the full model#
  if (AIC_reduced < best_AIC) {#
    best_AIC <- AIC_reduced#
    fullmodel <- reduced_model#
  } else {#
    # If AIC for the reduced model is not better, stop the loop#
    break#
  }#
}#
#
# Display the summary of the best model#
summary(fullmodel)
# Assuming 'fullmodel' is your initial model#
#
# Start with all variables in the model#
predictors <- names(fullmodel)#
#
# Perform backward elimination#
while (length(predictors) > 1) {#
  # Fit the model#
  current_model <- lm(formula(fullmodel), data = your_data_frame)#
  # Get p-values for each predictor#
  p_values <- summary(current_model)$coefficients[, 4]#
  # Identify predictor with the highest p-value#
  max_p_value_index <- which.max(p_values[-1]) # Exclude intercept#
  # If the highest p-value is greater than your threshold (e.g., 0.05), remove the predictor#
  if (p_values[max_p_value_index] > 0.05) {#
    predictors <- predictors[-(max_p_value_index + 1)] # +1 to account for intercept#
    formula_string <- paste("dependent_variable ~", paste(predictors, collapse = "+"))#
    fullmodel <- as.formula(formula_string)#
  } else {#
    break  # Break the loop if no predictor to remove#
  }#
}#
#
# Final model after backward elimination#
final_model <- lm(formula(fullmodel), data = your_data_frame)
backward_elimination <- function(model, threshold = 0.05) {#
  while(any(coef(summary(model))[, 4] > threshold)) {#
    max_p_value_index <- which.max(coef(summary(model))[, 4])#
    variables <- names(model$coefficients)#
    variable_to_remove <- variables[max_p_value_index]#
    model <- update(model, . ~ . - eval(parse(text = variable_to_remove)))#
  }#
  return(model)#
}#
#
# Initial model#
initial_model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Backward elimination#
final_model <- backward_elimination(initial_model)#
#
# Summary of the final model#
summary(final_model)
backward_elimination_aic <- function(model) {#
  initial_aic <- AIC(model)#
  while(TRUE) {#
    max_p_value_index <- which.max(coef(summary(model))[, 4])#
    variables <- names(model$coefficients)#
    variable_to_remove <- variables[max_p_value_index]#
    updated_formula <- as.formula(paste(". ~ . - ", variable_to_remove))#
    updated_model <- update(model, updated_formula)#
    if (AIC(updated_model) > initial_aic) {#
      break#
    }#
    model <- updated_model#
  }#
  return(model)#
}#
#
# Initial model#
initial_model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Backward elimination based on AIC#
final_model <- backward_elimination_aic(initial_model)#
#
# Summary of the final model#
summary(final_model)
backward_elimination_one_by_one <- function(model) {#
  initial_aic <- AIC(model)#
  while(TRUE) {#
    variables <- names(model$coefficients)[-1]  # Exclude intercept#
    aic_values <- numeric(length(variables))#
    for (i in seq_along(variables)) {#
      updated_formula <- as.formula(paste(". ~ . - ", variables[i]))#
      updated_model <- update(model, updated_formula)#
      aic_values[i] <- AIC(updated_model)#
    }#
    min_aic_index <- which.min(aic_values)#
    if (aic_values[min_aic_index] >= initial_aic) {#
      break#
    }#
    variable_to_remove <- variables[min_aic_index]#
    model <- update(model, . ~ . - eval(parse(text = variable_to_remove)))#
  }#
  return(model)#
}#
#
# Initial model#
initial_model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Backward elimination one by one based on AIC#
final_model <- backward_elimination_one_by_one(initial_model)#
#
# Summary of the final model#
summary(final_model)
# Initial model#
initial_model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
initial_aic <- AIC(initial_model)#
#
# Function to calculate AIC for a given model#
calculate_aic <- function(model) {#
  return(AIC(model))#
}#
#
# Function to remove one predictor based on AIC criterion#
backward_elimination_manual <- function(model) {#
  variables <- names(model$coefficients)[-1]  # Exclude intercept#
  aic_values <- numeric(length(variables))#
  for (i in seq_along(variables)) {#
    updated_formula <- as.formula(paste(". ~ . - ", variables[i]))#
    updated_model <- update(model, updated_formula)#
    aic_values[i] <- calculate_aic(updated_model)#
  }#
  min_aic_index <- which.min(aic_values)#
  if (aic_values[min_aic_index] < initial_aic) {#
    variable_to_remove <- variables[min_aic_index]#
    model <- update(model, . ~ . - eval(parse(text = variable_to_remove)))#
    print(paste("Removed predictor:", variable_to_remove))#
    print(paste("New AIC:", aic_values[min_aic_index]))#
    print(summary(model))#
    backward_elimination_manual(model)#
  } else {#
    print("No more predictors to remove.")#
    return(model)#
  }#
}#
#
# Perform backward elimination manually#
final_model_manual <- backward_elimination_manual(initial_model)
# Initial model#
initial_model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
initial_aic <- AIC(initial_model)#
#
# Function to calculate AIC for a given model#
calculate_aic <- function(model) {#
  return(AIC(model))#
}#
#
# Function to remove one predictor based on AIC criterion#
backward_elimination_manual <- function(model) {#
  variables <- names(model$coefficients)[-1]  # Exclude intercept#
  aic_values <- numeric(length(variables))#
  for (i in seq_along(variables)) {#
    updated_formula <- as.formula(paste(". ~ . - ", variables[i]))#
    updated_model <- update(model, updated_formula)#
    aic_values[i] <- calculate_aic(updated_model)#
  }#
  min_aic_index <- which.min(aic_values)#
  if (aic_values[min_aic_index] < initial_aic) {#
    variable_to_remove <- variables[min_aic_index]#
    model <- update(model, . ~ . - eval(parse(text = variable_to_remove)))#
    print(paste("Removed predictor:", variable_to_remove))#
    print(paste("New AIC:", aic_values[min_aic_index]))#
    print(summary(model))#
    backward_elimination_manual(model)#
  } else {#
    print("No more predictors to remove.")#
    return(model)#
  }#
}#
#
# Perform backward elimination manually#
final_model_manual <- backward_elimination_manual(initial_model)
x = summary(fullmodel)
x
x = summary(model)
x = summary(fullmodel)
x = summary(fullmodel)
names(x)
x$P-value
x$coefficients
y =	x$coefficients
z = y[,4]
z = y[,4];z
z = as.vector(y[,4])
z = as.vector(y[,4]);z
z = which.max(as.vector(y[,4]))
z = which.max(as.vector(y[,4]));z
x
x
reduced_data = training_data[-z+1]
names(reduced_data
)
reduced_data_1 = training_data[-z+1]
x = summary(fullmodel)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_1 = training_data[-z+1]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)
# =====================#
# Reduced Model -> 2#
# =====================#
x = summary(reduced_model_1)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_2 = reduced_data_1[-z+1]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)
summary(reduced_model_1)
# =====================#
# Reduced Model -> 3#
# =====================#
x = summary(reduced_model_2)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_3 = reduced_data_2[-z+1]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)
# =====================#
# Reduced Model -> 4#
# =====================#
x = summary(reduced_model_3)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_4 = reduced_data_3[-z+1]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)
x = summary(reduced_model_4)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_5 = reduced_data_4[-z+1]#
reduced_model_5 = glm(TenYearCHD ~ ., data = reduced_data_5, family = binomial(link = "logit"))#
summary(reduced_model_5)
x = summary(reduced_model_5)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_6 = reduced_data_5[-z+1]#
reduced_model_5 = glm(TenYearCHD ~ ., data = reduced_data_6, family = binomial(link = "logit"))#
summary(reduced_model_6)
x = summary(reduced_model_4)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_5 = reduced_data_4[-z+1]#
reduced_model_5 = glm(TenYearCHD ~ ., data = reduced_data_5, family = binomial(link = "logit"))#
summary(reduced_model_5)#
AIC_Reduced_Model_5 = 2131.6
x = summary(reduced_model_5)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_6 = reduced_data_5[-z+1]#
reduced_model_6 = glm(TenYearCHD ~ ., data = reduced_data_6, family = binomial(link = "logit"))#
summary(reduced_model_6)
# =====================#
# Reduced Model -> 7#
# =====================#
x = summary(reduced_model_6)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_7 = reduced_data_5[-z+1]#
reduced_model_7 = glm(TenYearCHD ~ ., data = reduced_data_7, family = binomial(link = "logit"))#
summary(reduced_model_7)
# =====================#
# Reduced Model -> 8#
# =====================#
x = summary(reduced_model_6)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_8 = reduced_data_7[-z+1]#
reduced_model_8 = glm(TenYearCHD ~ ., data = reduced_data_8, family = binomial(link = "logit"))#
summary(reduced_model_8)
# Selected_Model = reduced_model_6
Selected_Model = reduced_model_6
fitted_prob = fitted(Selected_Model)
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
fitted_prob
TenYearCHD
fitted_prob
TenYearCHD
length(TemYearCHD)
length(TenYearCHD)
length(Y.hat)
length(fitted_prob)
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,training_data$TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,training_data$TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
# Full Model#
fullmodel = glm(TenYearCHD ~ ., data = data, family = binomial(link = "logit"))#
AIC_full_model = 2139.9#
# =====================#
# Reduced Model -> 1#
# =====================#
x = summary(fullmodel)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_1 = training_data[-z+1]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)#
AIC_Reduced_Model_1 = 2138.2#
# =====================#
# Reduced Model -> 2#
# =====================#
x = summary(reduced_model_1)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_2 = reduced_data_1[-z+1]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_Reduced_Model_2 = 2136.3#
# =====================#
# Reduced Model -> 3#
# =====================#
x = summary(reduced_model_2)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_3 = reduced_data_2[-z+1]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)#
AIC_Reduced_Model_3 = 2134.7#
# =====================#
# Reduced Model -> 4#
# =====================#
x = summary(reduced_model_3)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_4 = reduced_data_3[-z+1]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)#
AIC_Reduced_Model_4 = 2133.2#
# =====================#
# Reduced Model -> 5#
# =====================#
x = summary(reduced_model_4)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_5 = reduced_data_4[-z+1]#
reduced_model_5 = glm(TenYearCHD ~ ., data = reduced_data_5, family = binomial(link = "logit"))#
summary(reduced_model_5)#
AIC_Reduced_Model_5 = 2131.6#
# =====================#
# Reduced Model -> 6#
# =====================#
x = summary(reduced_model_5)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_6 = reduced_data_5[-z+1]#
reduced_model_6 = glm(TenYearCHD ~ ., data = reduced_data_6, family = binomial(link = "logit"))#
summary(reduced_model_6)#
AIC_Reduced_Model_6 = 2130.1#
# =====================#
# Reduced Model -> 7#
# =====================#
x = summary(reduced_model_6)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_7 = reduced_data_5[-z+1]#
reduced_model_7 = glm(TenYearCHD ~ ., data = reduced_data_7, family = binomial(link = "logit"))#
summary(reduced_model_7)#
AIC_Reduced_Model_7 = 2130.4#
#
Selected_Model = reduced_model_6#
fitted_prob = fitted(Selected_Model)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
fitted_prob
fitted_prob = fitted(Selected_Model)
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
length(Y.Hat)
length(Y.hat)
length(TenYearCHD)
length(fitted_prob)
fitted_prob = fitted(full_model)
fitted_prob = fitted(fullmodel)
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.01)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
Selected_Model = fullmodel
fitted_prob = fitted(Selected_Model)
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.8,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.8,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_Thresold
optimum_thresold
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_thresold
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.999,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.95,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_thresold
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_thresold
Selected_Model = reduced_model_6
Selected_Model = reduced_model_6
fitted_prob = fitted(Selected_Model)
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
length(reduced_data_1)
length(reduced_data_2)
length(reduced_data_3)
nrows(reduced_data_1
)
nrow(reduced_data_1)
nrow(reduced_data_2)
nrow(reduced_data_3)
nrow(reduced_data_4)
nrow(reduced_data_5)
nrow(training_data)
Selected_Model = reduced_model_6#
fitted_prob = fitted(Selected_Model)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,training_data$TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_thresold
fitted_prob = fitted(Selected_Model)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,training_data$TenYearCHD)#
print(confusion_matrix)#
 TN = confusion_matrix[1, 1]  # True Negatives#
FP = confusion_matrix[1, 2]  # False Positives#
FN = confusion_matrix[2, 1]  # False Negatives#
TP = confusion_matrix[2, 2]  # True Positives#
#
TPR[k] = TP / (TP + FN)#
FPR[k] = FP / (TN + FP)#
#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]
optimum_thresold
TPR <- numeric()#
FPR <- numeric()#
Index <- numeric()#
k <- 1#
p <- seq(0.1, 0.9, 0.001)#
for (i in p) {#
  print(paste("Threshold = ", i))#
  Y.hat <- ifelse(fitted_prob > i, 1, 0)#
  confusion_matrix <- table(Y.hat, training_data$TenYearCHD)#
  print(confusion_matrix)#
  TN <- confusion_matrix[1, 1]  # True Negatives#
  FP <- confusion_matrix[1, 2]  # False Positives#
  FN <- confusion_matrix[2, 1]  # False Negatives#
  TP <- confusion_matrix[2, 2]  # True Positives#
  TPR[k] <- TP / (TP + FN)#
  FPR[k] <- FP / (TN + FP)#
  Index[k] <- TPR[k] * (1 - FPR[k])#
  plot(FPR, TPR, type = "l", main = "Finding Optimum Model . . . ")#
  k <- k + 1#
}#
optimum_threshold <- p[which.max(Index)]
library(caret)#
#
TPR <- numeric()#
FPR <- numeric()#
Index <- numeric()#
k <- 1#
p <- seq(0.1, 0.9, 0.001)#
#
for (i in p) {#
  print(paste("Threshold = ", i))#
  Y.hat <- ifelse(fitted_prob > i, 1, 0)#
  confusion_matrix <- confusionMatrix(factor(Y.hat), factor(training_data$TenYearCHD))#
  print(confusion_matrix$table)#
  TN <- confusion_matrix$table[1, 1]  # True Negatives#
  FP <- confusion_matrix$table[1, 2]  # False Positives#
  FN <- confusion_matrix$table[2, 1]  # False Negatives#
  TP <- confusion_matrix$table[2, 2]  # True Positives#
  TPR[k] <- TP / (TP + FN)#
  FPR[k] <- FP / (TN + FP)#
  Index[k] <- TPR[k] * (1 - FPR[k]) #
  plot(FPR, TPR, type = "l", main = "Finding Optimum Model . . . ")#
  k <- k + 1#
}#
#
optimum_threshold <- p[which.max(Index)]
#=============================================##
# Finding threshold by Optimising TPR*(1-FPR) ##
#=============================================##
TPR <- array()#
FPR <- array()#
Index <- array()#
k <- 1#
p <- seq(0.1, 0.9, 0.001)#
#
for (i in p) {#
  print(paste("Threshold = ", i))#
  Y.hat <- ifelse(fitted_prob > i, 1, 0)#
  confusion_matrix <- table(Y.hat, training_data$TenYearCHD)#
  print(confusion_matrix)#
  TN <- confusion_matrix[1, 1]  # True Negatives#
  FP <- confusion_matrix[1, 2]  # False Positives#
  FN <- confusion_matrix[2, 1]  # False Negatives#
  TP <- confusion_matrix[2, 2]  # True Positives#
  TPR[k] <- TP / (TP + FN)#
  FPR[k] <- FP / (TN + FP)#
  Index[k] <- TPR[k] * (1 - FPR[k]) #
  plot(FPR, TPR, type = "l", main = "Finding Optimum Model . . . ")#
  k <- k + 1#
}#
optimum_threshold <- p[which.max(Index)]
Y.hat
Y.hat=ifelse(fitted_prob>0.8,1,0)
Y.hat
fitted_prob = fitted(Selected_Model)#
#
#=============================================##
# Finding threshold by Optimizing TPR*(1-FPR)  ##
#=============================================##
#
roc_data = data.frame(fitted_prob, training_data$TenYearCHD)#
colnames(roc_data) = c('prob', 'chd')#
roc_data = roc_data[order(-roc_data$prob),]#
#
fpr <- numeric(length = length(roc_data$prob))#
tpr <- numeric(length = length(roc_data$prob))#
#
for(i in 1:length(roc_data$prob)){#
  fpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 0) / sum(roc_data$chd == 0)#
  tpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 1) / sum(roc_data$chd == 1)#
}#
#
roc_df = data.frame(fpr, tpr)#
#
roc_df$optimum <- roc_df$tpr * (1 - roc_df$fpr)#
optimal_threshold = roc_data$prob[which.max(roc_df$optimum)]#
#
# Plotting ROC curve#
ggplot(roc_df, aes(x = fpr, y = tpr)) +#
  geom_line() +#
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +#
  ggtitle("Receiver Operating Characteristic (ROC) Curve") +#
  xlab("False Positive Rate (FPR)") +#
  ylab("True Positive Rate (TPR)") +#
  geom_point(data = roc_df[which.max(roc_df$optimum), ], aes(x = fpr, y = tpr), color = "red", size = 3) +#
  annotate("text", x = roc_df[which.max(roc_df$optimum), ]$fpr, y = roc_df[which.max(roc_df$optimum), ]$tpr, label = paste("Threshold:", round(optimal_threshold, 2)), vjust = -1, hjust = 1, color = "red")
optimum
roc_df$optimum
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve#
plot(roc_curve, print.thres = "best", print.thres.best.method = "closest.topleft", main = "ROC Curve")#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red")
fitted_prob = fitted(Selected_Model)#
#
#=============================================##
# Finding threshold by Optimizing TPR*(1-FPR)  ##
#=============================================##
#
roc_data = data.frame(fitted_prob, training_data$TenYearCHD)#
colnames(roc_data) = c('prob', 'chd')#
roc_data = roc_data[order(-roc_data$prob),]#
#
fpr <- numeric(length = length(roc_data$prob))#
tpr <- numeric(length = length(roc_data$prob))#
#
for(i in 1:length(roc_data$prob)){#
  fpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 0) / sum(roc_data$chd == 0)#
  tpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 1) / sum(roc_data$chd == 1)#
}#
#
roc_df = data.frame(fpr, tpr)#
#
roc_df$optimum <- roc_df$tpr * (1 - roc_df$fpr)#
optimal_threshold = roc_data$prob[which.max(roc_df$optimum)]#
#
# Plotting ROC curve#
ggplot(roc_df, aes(x = fpr, y = tpr)) +#
  geom_line() +#
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +#
  ggtitle("Receiver Operating Characteristic (ROC) Curve") +#
  xlab("False Positive Rate (FPR)") +#
  ylab("True Positive Rate (TPR)") +#
  geom_point(data = roc_df[which.max(roc_df$optimum), ], aes(x = fpr, y = tpr), color = "red", size = 3) +#
  annotate("text", x = roc_df[which.max(roc_df$optimum), ]$fpr, y = roc_df[which.max(roc_df$optimum), ]$tpr, label = paste("Threshold:", round(optimal_threshold, 2)), vjust = -1, hjust = 1, color = "red")
fitted_prob = fitted(Selected_Model)#
#
#=============================================##
# Finding threshold by Optimizing TPR*(1-FPR)  ##
#=============================================##
#
roc_data = data.frame(fitted_prob, training_data$TenYearCHD)#
colnames(roc_data) = c('prob', 'chd')#
roc_data = roc_data[order(-roc_data$prob),]#
#
fpr <- numeric(length = length(roc_data$prob))#
tpr <- numeric(length = length(roc_data$prob))#
#
for(i in 1:length(roc_data$prob)){#
  fpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 0) / sum(roc_data$chd == 0)#
  tpr[i] <- sum(roc_data$chd[i:length(roc_data$prob)] == 1) / sum(roc_data$chd == 1)#
}#
#
roc_df = data.frame(fpr, tpr)#
#
roc_df$optimum <- roc_df$tpr * (1 - roc_df$fpr)#
optimal_threshold = roc_data$prob[which.max(roc_df$optimum)]#
#
# Plotting ROC curve#
ggplot(roc_df, aes(x = fpr, y = tpr)) +#
  geom_line() +#
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +#
  ggtitle("Receiver Operating Characteristic (ROC) Curve") +#
  xlab("False Positive Rate (FPR)") +#
  ylab("True Positive Rate (TPR)") +#
  geom_point(data = roc_df[which.max(roc_df$optimum), ], aes(x = fpr, y = tpr), color = "red", size = 3) +#
  annotate("text", x = roc_df[which.max(roc_df$optimum), ]$fpr, y = roc_df[which.max(roc_df$optimum), ]$tpr, label = paste("Threshold:", round(optimal_threshold, 2)), vjust = -1, hjust = 1, color = "red")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve#
plot(roc_curve, print.thres = "best", print.thres.best.method = "closest.topleft", main = "ROC Curve")#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve#
plot(roc_curve, print.thres = "best", print.thres.best.method = "closest.topleft", main = "ROC Curve")#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve#
plot(roc_curve, print.thres = "best", print.thres.best.method = "closest.topleft", main = "ROC Curve")#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2)#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
FPR
# Load pROC package#
library(pROC)#
#
# Calculate ROC curve#
roc_curve <- roc(roc_data$chd, roc_data$prob)#
#
# Find optimal threshold#
optimal_threshold <- coords(roc_curve, "best", ret = "threshold")#
#
# Plot ROC curve with customizations#
plot(roc_curve, #
     print.thres = "best", #
     print.thres.best.method = "closest.topleft", #
     main = "ROC Curve", #
     xlab = "False Positive Rate (FPR)", #
     ylab = "True Positive Rate (TPR)", #
     col = "blue", #
     lwd = 2,#
     rev = "tpr")  # Ensure FPR values are in increasing order#
#
# Add optimal threshold to the plot#
abline(v = optimal_threshold, col = "red", lty = 2)#
text(optimal_threshold, 0.5, labels = paste("Optimal Threshold:", round(optimal_threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Add legend#
legend("bottomright", #
       legend = c("ROC Curve", "Optimal Threshold"), #
       col = c("blue", "red"), #
       lwd = c(2, 1), #
       cex = 0.8, #
       bg = "white")
FPR=sort(FPR)
plot(FPR,TPR,type="l")
plot(roc_curve)
library(pROC)#
#
# Fit the logistic regression model#
model <- glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# Predict probabilities on the test set#
predicted_probs <- predict(model, newdata = testing_data, type = "response")#
#
# Calculate ROC curve#
roc_curve <- roc(testing_data$TenYearCHD, predicted_probs)#
#
# Plot ROC curve#
plot(roc_curve, main = "ROC Curve", percent = TRUE, xlab = "False Positive Rate", ylab = "True Positive Rate", col = "#377eb8", lwd = 2)#
#
# Add optimal threshold to the plot#
coords <- coords(roc_curve, "best", ret = c("threshold", "specificity", "sensitivity"))#
abline(v = coords$threshold, col = "red", lty = 2)#
text(coords$threshold, coords$sensitivity, labels = paste("Optimal Threshold:", round(coords$threshold, 2)), pos = 2, col = "red", cex = 0.8)#
#
# Print AUC#
print(roc_curve, print.auc = TRUE)
# ======================================================== ##
#                    Final Code                            ##
# ======================================================== ##
rm(list=ls())#
#install.packages("lattice")#
# install.packages("caret")#
#install.packages("pROC")#
#install.packages("randomForest")#
#install.packages("olsrr")#
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
attach(data)#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]#
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
# = = = = = = = = = = = #
# Correlation Heatmap#
# = = = = = = = = = = = #
correlation_matrix = cor(data)#
melted_correlation = melt(correlation_matrix)#
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +#
  geom_tile() + #
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +#
  scale_fill_gradient2(low = "#58390b", high = "#0f423c", midpoint = 0, limit = c(-1,1), name="Correlation", mid = "#FFFFFF") +#
  theme_minimal() +#
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +#
  coord_fixed() +#
  labs(x = "Variables", y = "Variables")#
# = = = = = = = = = = = = =#
# Converting into factors#
# = = = = = = = = = = = = =#
data$male = as.factor(data$male)#
data$education = as.factor(data$education)#
data$currentSmoker = as.factor(data$currentSmoker)#
data$prevalentStroke = as.factor(data$prevalentStroke)#
data$prevalentHyp = as.factor(data$prevalentHyp)#
data$diabetes = as.factor(data$diabetes)#
data$TenYearCHD = as.factor(data$TenYearCHD)#
# = = = = = = = = = = = = = #
# Plottting the VIF Values#
# = = = = = = = = = = = = =#
#
vif_values = vif(fullmodel)#
vif_df = data.frame(Variable = names(vif_values), VIF = unname(vif_values))#
#
# Create the bar chart using ggplot#
ggplot(vif_df, aes(x = Variable, y = VIF, fill = VIF)) +#
  geom_bar(stat = "identity", color = "steelblue") +#
  geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +#
  theme_minimal() +#
  labs(title = "Plotting of the VIF Values", y = "VIF Value", x = "") +#
  coord_cartesian(ylim = c(0, max(vif_df$VIF) + 1)) + # Adjust y-axis limits#
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Angle the x-axis text#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#  Code for testing the significance of the predictor variables#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
x = summary(fullmodel)#
y = x$coefficients#
estimates = y[,1][-c(1,18)]#
se = y[,2][-c(1,18)]#
walds_t = estimates/sqrt(se)#
significance = numeric(16)#
for(i in 1:16){#
	if(abs(walds_t[i])>1.96){#
		significance[i]= "Not Significant"#
	}else{#
		significance[i]="Significant"#
	}#
}#
data.frame(names(training_data),estimates,se,walds_t,significance)#
#Findig Out the Odds Ratio#
Odds_Ratio = exp(estimates)#
p_values = 2*(1- qnorm(estimates)) ; p_values#
data.frame(names(training_data),exp(Odds_Ratio),p_values)#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
# Selection of the Best Model#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#
# Full Model#
AIC_full_model = 2095.7#
x = summary(fullmodel)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_1 = data[,-q] #
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)#
AIC_1 = 2784.8#
# Reduced Model - 01#
x = summary(reduced_model_1)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_2 = reduced_data_1[,-q]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_2 = 2786.8#
# Reduced Model - 02#
x = summary(reduced_model_2)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_3 = reduced_data_2[,-q]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)#
AIC_3 = 2782.9#
# Reduced Model - 03#
x = summary(reduced_model_3)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_4 = reduced_data_3[,-q]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)#
AIC_4 = 2791.6#
fitted_prob = fitted(reduced_model_3)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.0001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]#
#===================================##
#Checking the Model Accuracy        ##
#===================================##
#
binary_predictions = ifelse(fitted_prob > optimum_thresold, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
# ======================================================== ##
#                    Final Code                            ##
# ======================================================== ##
rm(list=ls())#
#install.packages("lattice")#
# install.packages("caret")#
#install.packages("pROC")#
#install.packages("randomForest")#
#install.packages("olsrr")#
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
attach(data)#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]#
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
# = = = = = = = = = = = #
# Correlation Heatmap#
# = = = = = = = = = = = #
correlation_matrix = cor(data)#
melted_correlation = melt(correlation_matrix)#
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +#
  geom_tile() + #
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +#
  scale_fill_gradient2(low = "#58390b", high = "#0f423c", midpoint = 0, limit = c(-1,1), name="Correlation", mid = "#FFFFFF") +#
  theme_minimal() +#
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +#
  coord_fixed() +#
  labs(x = "Variables", y = "Variables")#
# = = = = = = = = = = = = =#
# Converting into factors#
# = = = = = = = = = = = = =#
data$male = as.factor(data$male)#
data$education = as.factor(data$education)#
data$currentSmoker = as.factor(data$currentSmoker)#
data$prevalentStroke = as.factor(data$prevalentStroke)#
data$prevalentHyp = as.factor(data$prevalentHyp)#
data$diabetes = as.factor(data$diabetes)#
data$TenYearCHD = as.factor(data$TenYearCHD)#
# = = = = = = = = = = = = = #
# Plottting the VIF Values#
# = = = = = = = = = = = = =#
#
vif_values = vif(fullmodel)#
vif_df = data.frame(Variable = names(vif_values), VIF = unname(vif_values))#
#
# Create the bar chart using ggplot#
ggplot(vif_df, aes(x = Variable, y = VIF, fill = VIF)) +#
  geom_bar(stat = "identity", color = "steelblue") +#
  geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +#
  theme_minimal() +#
  labs(title = "Plotting of the VIF Values", y = "VIF Value", x = "") +#
  coord_cartesian(ylim = c(0, max(vif_df$VIF) + 1)) + # Adjust y-axis limits#
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Angle the x-axis text#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#  Code for testing the significance of the predictor variables#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
x = summary(fullmodel)#
y = x$coefficients#
estimates = y[,1][-c(1,18)]#
se = y[,2][-c(1,18)]#
walds_t = estimates/sqrt(se)#
significance = numeric(16)#
for(i in 1:16){#
	if(abs(walds_t[i])>1.96){#
		significance[i]= "Not Significant"#
	}else{#
		significance[i]="Significant"#
	}#
}#
data.frame(names(training_data),estimates,se,walds_t,significance)#
#Findig Out the Odds Ratio#
Odds_Ratio = exp(estimates)#
p_values = 2*(1- qnorm(estimates)) ; p_values#
data.frame(names(training_data),exp(Odds_Ratio),p_values)#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
# Selection of the Best Model#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#
# Full Model#
AIC_full_model = 2095.7#
x = summary(fullmodel)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_1 = data[,-q] #
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)#
AIC_1 = 2784.8#
# Reduced Model - 01#
x = summary(reduced_model_1)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_2 = reduced_data_1[,-q]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_2 = 2786.8#
# Reduced Model - 02#
x = summary(reduced_model_2)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_3 = reduced_data_2[,-q]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)#
AIC_3 = 2782.9#
# Reduced Model - 03#
x = summary(reduced_model_3)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_4 = reduced_data_3[,-q]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)#
AIC_4 = 2791.6#
fitted_prob = fitted(reduced_model_3)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,1,0.01)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]#
#===================================##
#Checking the Model Accuracy        ##
#===================================##
#
binary_predictions = ifelse(fitted_prob > optimum_thresold, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
# ======================================================== ##
#                    Final Code                            ##
# ======================================================== ##
rm(list=ls())#
#install.packages("lattice")#
# install.packages("caret")#
#install.packages("pROC")#
#install.packages("randomForest")#
#install.packages("olsrr")#
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
attach(data)#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]#
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
# = = = = = = = = = = = #
# Correlation Heatmap#
# = = = = = = = = = = = #
correlation_matrix = cor(data)#
melted_correlation = melt(correlation_matrix)#
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +#
  geom_tile() + #
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +#
  scale_fill_gradient2(low = "#58390b", high = "#0f423c", midpoint = 0, limit = c(-1,1), name="Correlation", mid = "#FFFFFF") +#
  theme_minimal() +#
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +#
  coord_fixed() +#
  labs(x = "Variables", y = "Variables")#
# = = = = = = = = = = = = =#
# Converting into factors#
# = = = = = = = = = = = = =#
data$male = as.factor(data$male)#
data$education = as.factor(data$education)#
data$currentSmoker = as.factor(data$currentSmoker)#
data$prevalentStroke = as.factor(data$prevalentStroke)#
data$prevalentHyp = as.factor(data$prevalentHyp)#
data$diabetes = as.factor(data$diabetes)#
data$TenYearCHD = as.factor(data$TenYearCHD)#
# = = = = = = = = = = = = = #
# Plottting the VIF Values#
# = = = = = = = = = = = = =#
#
vif_values = vif(fullmodel)#
vif_df = data.frame(Variable = names(vif_values), VIF = unname(vif_values))#
#
# Create the bar chart using ggplot#
ggplot(vif_df, aes(x = Variable, y = VIF, fill = VIF)) +#
  geom_bar(stat = "identity", color = "steelblue") +#
  geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +#
  theme_minimal() +#
  labs(title = "Plotting of the VIF Values", y = "VIF Value", x = "") +#
  coord_cartesian(ylim = c(0, max(vif_df$VIF) + 1)) + # Adjust y-axis limits#
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Angle the x-axis text#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#  Code for testing the significance of the predictor variables#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
x = summary(fullmodel)#
y = x$coefficients#
estimates = y[,1][-c(1,18)]#
se = y[,2][-c(1,18)]#
walds_t = estimates/sqrt(se)#
significance = numeric(16)#
for(i in 1:16){#
	if(abs(walds_t[i])>1.96){#
		significance[i]= "Not Significant"#
	}else{#
		significance[i]="Significant"#
	}#
}#
data.frame(names(training_data),estimates,se,walds_t,significance)#
#Findig Out the Odds Ratio#
Odds_Ratio = exp(estimates)#
p_values = 2*(1- qnorm(estimates)) ; p_values#
data.frame(names(training_data),exp(Odds_Ratio),p_values)#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
# Selection of the Best Model#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#
# Full Model#
AIC_full_model = 2095.7#
x = summary(fullmodel)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_1 = data[,-q] #
reduced_model_1 =  glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)#
AIC_1 = 2784.8#
# Reduced Model - 01#
x = summary(reduced_model_1)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_2 = reduced_data_1[,-q]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_2 = 2786.8#
# Reduced Model - 02#
x = summary(reduced_model_2)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_3 = reduced_data_2[,-q]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)#
AIC_3 = 2782.9#
# Reduced Model - 03#
x = summary(reduced_model_3)#
y = x$coefficients#
z = as.data.frame(y[,4])#
p = z[,]#
q = which.max(p) ; q#
reduced_data_4 = reduced_data_3[,-q]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)#
AIC_4 = 2791.6#
fitted_prob = fitted(reduced_model_3)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.01)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]#
#===================================##
#Checking the Model Accuracy        ##
#===================================##
#
binary_predictions = ifelse(fitted_prob > optimum_thresold, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
optimum_thresold
binary_predictions = ifelse(fitted_prob > optimum_thresold, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
binary_predictions = ifelse(fitted_prob > 0.15, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
binary_predictions = ifelse(fitted_prob > 0.14, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
confusion_matrix = table(binary_predictions, training_data$TenYearCHD)
Selected_Model = fullmodel
fitted_prob = fitted(Selected_Model)
binary_predictions = ifelse(fitted_prob > 0.14, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
binary_predictions = ifelse(fitted_prob > 0.17, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
binary_predictions = ifelse(fitted_prob > 0.6, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
binary_predictions = ifelse(fitted_prob > 1, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
# ======================================================== ##
#                    Final Code                            ##
# ======================================================== ##
rm(list=ls())#
#install.packages("lattice")#
# install.packages("caret")#
#install.packages("pROC")#
#install.packages("randomForest")#
#install.packages("olsrr")#
library(car)#
library(caTools)#
library(ggplot2)#
library(pROC)#
library(reshape2)#
library(randomForest)#
library(InformationValue)#
library(olsrr)#
set.seed(1234)#
#
data = read.csv("/Users/suchibratapatra/Desktop/Dissertation/maindata.csv")#
split = sample.split(data, SplitRatio = 0.8)#
training_data = data[split, ]#
testing_data = data[!split, ]#
# = = = = = = = = = = = #
# Correlation Heatmap#
# = = = = = = = = = = = #
correlation_matrix = cor(data)#
melted_correlation = melt(correlation_matrix)#
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +#
  geom_tile() + #
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +#
  scale_fill_gradient2(low = "#58390b", high = "#0f423c", midpoint = 0, limit = c(-1,1), name="Correlation", mid = "#FFFFFF") +#
  theme_minimal() +#
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +#
  coord_fixed() +#
  labs(x = "Variables", y = "Variables")#
# = = = = = = = = = = = = =#
# Converting into factors#
# = = = = = = = = = = = = =#
data$male = as.factor(data$male)#
data$education = as.factor(data$education)#
data$currentSmoker = as.factor(data$currentSmoker)#
data$prevalentStroke = as.factor(data$prevalentStroke)#
data$prevalentHyp = as.factor(data$prevalentHyp)#
data$diabetes = as.factor(data$diabetes)#
data$TenYearCHD = as.factor(data$TenYearCHD)#
model = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
#
# = = = = = = = = = = = = = #
# Plottting the VIF Values#
# = = = = = = = = = = = = =#
#
vif_values = vif(model)#
vif_df = data.frame(Variable = names(vif_values), VIF = unname(vif_values))#
#
# Create the bar chart using ggplot#
ggplot(vif_df, aes(x = Variable, y = VIF, fill = VIF)) +#
  geom_bar(stat = "identity", color = "steelblue") +#
  geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +#
  theme_minimal() +#
  labs(title = "Plotting of the VIF Values", y = "VIF Value", x = "") +#
  coord_cartesian(ylim = c(0, max(vif_df$VIF) + 1)) + # Adjust y-axis limits#
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Angle the x-axis text#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#  Code for testing the significance of the predictor variables#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
#
x = summary(model)#
y = x$coefficients#
estimates = y[,1][-c(1,18)]#
se = y[,2][-c(1,18)]#
walds_t = estimates/sqrt(se)#
significance = numeric(16)#
for(i in 1:16){#
	if(abs(walds_t[i])>1.96){#
		significance[i]= "Not Significant"#
	}else{#
		significance[i]="Significant"#
	}#
}#
data.frame(names(training_data),estimates,se,walds_t,significance)#
#Findig Out the Odds Ratio#
Odds_Ratio = exp(estimates)#
p_values = 2*(1- qnorm(estimates)) ; p_values#
data.frame(names(training_data),exp(Odds_Ratio),p_values)#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
# Selection of the Best Model#
# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #
# Full Model#
fullmodel = glm(TenYearCHD ~ ., data = training_data, family = binomial(link = "logit"))#
AIC_full_model = 2139.9#
# =====================data#
# Reduced Model -> 1#
# =====================#
x = summary(fullmodel)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_1 = training_data[-z+1]#
reduced_model_1 = glm(TenYearCHD ~ ., data = reduced_data_1, family = binomial(link = "logit"))#
summary(reduced_model_1)#
AIC_Reduced_Model_1 = 2138.2#
# =====================#
# Reduced Model -> 2#
# =====================#
x = summary(reduced_model_1)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_2 = reduced_data_1[-z+1]#
reduced_model_2 = glm(TenYearCHD ~ ., data = reduced_data_2, family = binomial(link = "logit"))#
summary(reduced_model_2)#
AIC_Reduced_Model_2 = 2136.3#
# =====================#
# Reduced Model -> 3#
# =====================#
x = summary(reduced_model_2)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_3 = reduced_data_2[-z+1]#
reduced_model_3 = glm(TenYearCHD ~ ., data = reduced_data_3, family = binomial(link = "logit"))#
summary(reduced_model_3)#
AIC_Reduced_Model_3 = 2134.7#
# =====================#
# Reduced Model -> 4#
# =====================#
x = summary(reduced_model_3)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_4 = reduced_data_3[-z+1]#
reduced_model_4 = glm(TenYearCHD ~ ., data = reduced_data_4, family = binomial(link = "logit"))#
summary(reduced_model_4)#
AIC_Reduced_Model_4 = 2133.2#
# =====================#
# Reduced Model -> 5#
# =====================#
x = summary(reduced_model_4)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_5 = reduced_data_4[-z+1]#
reduced_model_5 = glm(TenYearCHD ~ ., data = reduced_data_5, family = binomial(link = "logit"))#
summary(reduced_model_5)#
AIC_Reduced_Model_5 = 2131.6#
# =====================#
# Reduced Model -> 6#
# =====================#
x = summary(reduced_model_5)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_6 = reduced_data_5[-z+1]#
reduced_model_6 = glm(TenYearCHD ~ ., data = reduced_data_6, family = binomial(link = "logit"))#
summary(reduced_model_6)#
AIC_Reduced_Model_6 = 2130.1#
# =====================#
# Reduced Model -> 7#
# =====================#
x = summary(reduced_model_6)#
y =	x$coefficients#
z = which.max(as.vector(y[,4]))#
reduced_data_7 = reduced_data_5[-z+1]#
reduced_model_7 = glm(TenYearCHD ~ ., data = reduced_data_7, family = binomial(link = "logit"))#
summary(reduced_model_7)#
AIC_Reduced_Model_7 = 2130.4#
#
Selected_Model = fullmodel#
fitted_prob = fitted(Selected_Model)#
#=============================================##
# Finding thresold by Optimising TPR*(1-FPR)  ##
#=============================================##
TPR=array()#
FPR=array()#
Index = array()#
k=1#
p=seq(0.1,0.9,0.001)#
for(i in p)#
{#
print(paste("Threshold = ",i))#
Y.hat=ifelse(fitted_prob>i,1,0)#
confusion_matrix = table(Y.hat,TenYearCHD)#
print(confusion_matrix)#
  TN = confusion_matrix[1, 1]  # True Negatives#
  FP = confusion_matrix[1, 2]  # False Positives#
  FN = confusion_matrix[2, 1]  # False Negatives#
  TP = confusion_matrix[2, 2]  # True Positives#
  TPR[k] = TP / (TP + FN)#
  FPR[k] = FP / (TN + FP + FN + TP)#
  Index[k] = TPR[k] * (1 - FPR[k]) #
  plot(FPR,TPR,type="l",main="Finding Optimum Model . . . ")#
  k=k+1#
}#
optimum_thresold = p[which.max(Index)]#
#===================================##
#Checking the Model Accuracy        ##
#===================================##
#
binary_predictions = ifelse(fitted_prob > optimum_thresold, 1, 0)#
#
# Calculate confusion matrix#
confusion_matrix = table(binary_predictions, TenYearCHD)#
#
# Print confusion matrix#
print("Confusion Matrix:")#
print(confusion_matrix)#
#
# Calculate accuracy#
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)#
print(paste("Accuracy:", round(accuracy, 3)))#
#
# Calculate precision#
precision = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])#
print(paste("Precision:", round(precision, 3)))#
#
# Calculate recall (True Positive Rate)#
recall = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])#
print(paste("Recall (True Positive Rate):", round(recall, 3)))#
#
# Calculate F1-score#
F1_score = 2 * (precision * recall) / (precision + recall)#
print(paste("F1-score:", round(F1_score, 3)))
